{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree\n",
    "\n",
    "dump_file = 'wikipedia_2000_dump.xml'\n",
    "tree = xml.etree.ElementTree.parse(dump_file)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages count = 2000\n"
     ]
    }
   ],
   "source": [
    "def extract_alpha_words(page):\n",
    "    words = page.text.split()\n",
    "    alpha_words = list(filter(lambda word: word.isalpha(), words))\n",
    "    alpha_lower_words = list(map(lambda word: word.lower(), alpha_words))\n",
    "    return alpha_lower_words\n",
    "\n",
    "pages_words = []\n",
    "for page in root:\n",
    "    pages_words.append(extract_alpha_words(page))\n",
    "pages_count = len(pages_words)\n",
    "print('Pages count =', pages_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique alpha words = 210492\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "word_count = 0\n",
    "for page_words in pages_words:\n",
    "    for word in page_words:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = word_count\n",
    "            word_count += 1\n",
    "print('Unique alpha words =', word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random words' sample:\n",
      "бризовой\n",
      "инкоси\n",
      "русское\n",
      "прорвался\n",
      "поддержавшим\n",
      "голланца\n",
      "градусам\n",
      "первозданной\n",
      "далекие\n",
      "мексиканец\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(\"Random words' sample:\")\n",
    "words = list(word_dict.keys())\n",
    "sample = random.sample(range(len(words)), 10)\n",
    "for index in sample:\n",
    "    print(words[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.zeros((pages_count, word_count), dtype=np.float32)\n",
    "for index, page_words in enumerate(pages_words):\n",
    "    for word in page_words:\n",
    "        X[index][word_dict[word]] += (1 / len(page_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2000, M = 210492\n",
      "cov_matrix shape is (2000, 2000)\n",
      "Entropy is = 4.495633504928133\n",
      "Effective rank = 89.62492883852647\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def compute_erank(X):\n",
    "    N, M = X.shape\n",
    "    print('N = {}, M = {}'.format(N, M))\n",
    "    C = X.dot(X.T)\n",
    "    print('cov_matrix shape is {}'.format(C.shape))\n",
    "    _, s, _ = np.linalg.svd(C)\n",
    "    s_sum = np.sum(s)\n",
    "    p = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        p[i] = s[i] / s_sum\n",
    "    entropy = 0\n",
    "    for i in range(N):\n",
    "        entropy -= p[i] * math.log(p[i])\n",
    "    print('Entropy is = {}'.format(entropy))\n",
    "    erank = math.exp(entropy)\n",
    "    return erank\n",
    "\n",
    "print('Effective rank =', compute_erank(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
